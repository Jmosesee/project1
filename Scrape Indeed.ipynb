{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_job_links_page(page):\n",
    "    base_url = \"https://www.indeed.com/jobs?\"\n",
    "    params = {'q': 'data scientist',\n",
    "             'l': 'Denver, CO'}\n",
    "\n",
    "    # start = \"https://www.indeed.com/jobs?q=Data+Scientist&l=Denver%2C+CO\"\n",
    "    # use a fake header\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n",
    "    params['start'] = 10 * (page-1)\n",
    "\n",
    "    page = requests.get(base_url, params=params, headers=headers)\n",
    "    # test = requests.get(start, headers=headers)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    \n",
    "    # build a list of links\n",
    "    some_links = []\n",
    "\n",
    "    for l in links:\n",
    "        try:\n",
    "            hyperlink = l.attrs.get('href')\n",
    "            if \"/rc/clk?\" in hyperlink:\n",
    "                some_links.append(l.attrs.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    job_links = [\"https://www.indeed.com{}\".format(x)\n",
    "             for x in some_links\n",
    "             ]\n",
    "    \n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_summary(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}\n",
    "    test = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(test.text, \"html.parser\")\n",
    "    spans = soup.find_all('span')\n",
    "    spans_w_divs = [span.find_all('div') for span in spans if len(span.find_all('div')) > 0]\n",
    "    span = soup.find(\"span\", id=\"job_summary\")\n",
    "    return str(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def initialize_highlighting(filename):\n",
    "# Read a list of skill phrases from a file\n",
    "    skill_phrases = []\n",
    "    with open(filename, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            skill_phrases.append(str(*row).strip())\n",
    "    skill_phrases = set(skill_phrases)\n",
    "\n",
    "    # Write this clean version back out\n",
    "    with open('skill_phrases_out.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        for skill_phrase in skill_phrases:\n",
    "            csv_writer.writerow([skill_phrase])\n",
    "        \n",
    "    #separate each skill phrase into a list of its words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    skill_phrase_wl = [tokenizer.tokenize(skill_phrase) for skill_phrase in skill_phrases]\n",
    "    \n",
    "    return skill_phrase_wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_skill_mentions_in_job(t, skill_phrase_wl):\n",
    "    \n",
    "    skill_mentions_in_job = defaultdict(int)\n",
    "    # tokenize the text of the description, without spans\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    tokens = tokenizer.tokenize(t)\n",
    "    # create a dictionary of the words in the job description\n",
    "    word_index = defaultdict(list)\n",
    "    for i, k in enumerate(tokens):\n",
    "        word_index[k].append(i)\n",
    "    \n",
    "    # search the word_index dictionary to find the fist word of each skill_phrase\n",
    "    for skill_phrase in skill_phrase_wl:\n",
    "        if word_index.get(skill_phrase[0]):\n",
    "            for occurence in word_index.get(skill_phrase[0]):\n",
    "                # Check to see if the whole phrase matches\n",
    "                if all((skill_phrase[j] == tokens[j+occurence]) for j in range(len(skill_phrase))):\n",
    "                    skill_mentions_in_job[tuple(skill_phrase)] += 1\n",
    "    return skill_mentions_in_job        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# requires the global variable skill_phrase_wl\n",
    "# maybe this should be a parameter rather than a global\n",
    "def highlight_phrases_from_list(t):\n",
    "\n",
    "    # tokenize the text of the description, with spans\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9#+-]+')\n",
    "    span_generator = tokenizer.span_tokenize(t)\n",
    "    spans = [span for span in span_generator]\n",
    "    tokens = [t[span[0]:span[1]] for span in spans]\n",
    "    \n",
    "#     # create a dictionary of the words, with spans as the values\n",
    "#     # and another dictionary with the same keys, with the word indexes as the values\n",
    "    char_span = defaultdict(list)\n",
    "    word_index = defaultdict(list)\n",
    "    for i, (k, span) in enumerate(zip(tokens, spans)):\n",
    "        char_span[k].append(span)\n",
    "        word_index[k].append(i)\n",
    "    # is this useful?\n",
    "    df = pd.DataFrame({'Character Index Spans': pd.Series(char_span), 'Word Indexes': pd.Series(word_index)})\n",
    "\n",
    "    highlight_spans = []   \n",
    "    for skill_phrase in skill_phrase_wl:\n",
    "        if word_index.get(skill_phrase[0]):\n",
    "            for i, occurence in enumerate(word_index.get(skill_phrase[0])):\n",
    "                if all((skill_phrase[j] == tokens[j+occurence]) for j in range(len(skill_phrase))):\n",
    "                    highlight_span = (spans[occurence][0], spans[occurence + len(skill_phrase) - 1][1])\n",
    "                    highlight_spans.append (highlight_span)\n",
    "\n",
    "# # look up the words in our skill list in the dictionary.  List the findings as spans to be highlighted\n",
    "#     for skill in single_word_skills:\n",
    "#         highlight_spans += char_span[skill]\n",
    "\n",
    "    # Sort the spans to be highlighted\n",
    "    highlight_spans.sort()\n",
    "\n",
    "    # Insert html tags to highlight the keywords\n",
    "    html_start_tag = '<font color=\"red\">'\n",
    "    html_end_tag = '</font>'\n",
    "    highlighted = ''\n",
    "    cursor = 0\n",
    "    for span in highlight_spans:\n",
    "        if (span[0] > cursor): # go forwards only, not backwards \n",
    "            if (cursor>0):\n",
    "                highlighted += html_end_tag\n",
    "            highlighted += t[cursor:span[0]] + \\\n",
    "                            html_start_tag + \\\n",
    "                            t[span[0]:span[1]]\n",
    "        elif (span[1] > cursor):\n",
    "            highlighted += t[cursor:span[1]]\n",
    "        cursor = span[1]\n",
    "    highlighted += html_end_tag + t[cursor:]\n",
    "    display(HTML(highlighted))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_links = []\n",
    "for page in range(1,25):\n",
    "    job_links+=(get_job_links_page(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████████████████████████▊                                  | 127/222 [00:53<00:39,  2.40it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tallied_skill_mentions = []\n",
    "skill_phrase_wl = initialize_highlighting('skill_phrases.csv')\n",
    "skill_dict = {tuple(skill_phrase): 0 for skill_phrase in skill_phrase_wl}\n",
    "for link in tqdm(job_links):\n",
    "    # highlight_phrases_from_list(get_job_summary(link))\n",
    "    tallied_skill_mentions.append(tally_skill_mentions_in_job(get_job_summary(link), skill_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tallied_skill_mentions, index = job_links).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_mentioned = df.columns.values\n",
    "skill_phrases_mentioned = [' '.join(c) for c in df.columns.values]\n",
    "df.columns = skill_phrases_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_mentioned = skill_dict\n",
    "for skill in skills_mentioned:\n",
    "    del not_mentioned[skill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_not_mentioned = [' '.join(s) for s in not_mentioned]\n",
    "phrases_not_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
